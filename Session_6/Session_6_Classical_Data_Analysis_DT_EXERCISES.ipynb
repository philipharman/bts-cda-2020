{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aB_ck5OSE3TU"
   },
   "source": [
    "![BTS](https://github.com/vfp1/bts-mbds-data-science-foundations-2019/raw/master/sessions/img/Logo-BTS.jpg)\n",
    "\n",
    "# Session 6: Decision Trees EXERCISES\n",
    "\n",
    "\n",
    "### Filipa Peleja <filipa.peleja@bts.tech>\n",
    "### Victor F. Pajuelo Madrigal <victor.pajuelo@bts.tech>\n",
    "\n",
    "## Classical Data Analysis (22-02-2022)\n",
    "\n",
    "Open this notebook in Google Colaboratory: [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vfp1/bts-cda-2020/blob/main/Session_6/Session_6_Classical_Data_Analysis_DT_EXERCISES.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_dCdQvVLm8DY"
   },
   "source": [
    "## EXERCISE 1 - Decision Trees and Cross Validation\n",
    "\n",
    "Follow the steps to find the best parameters on the Decision Tree classification over a Moon dataset. Try to play with the hyperparameters and reason your results with theory in hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DewyrECuR7kG"
   },
   "source": [
    "### Generate a moons dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RRdATwBGPAs2"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=10000, noise=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iBer2I-FSJ3G"
   },
   "source": [
    "### Split the train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "4WTj4rzaSVC_",
    "outputId": "49927a3f-64b2-4879-88da-8ac4cd46f667"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ejwKmbqqSXoJ"
   },
   "source": [
    "### Use grid search with Cross Validation\n",
    "Try to find good hyperparameter values for a DecisionTreeClassifier. Hint: try various values for max_leaf_nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "colab_type": "code",
    "id": "XwHRu9CkSbnH",
    "outputId": "e1f12ce7-168f-4954-91fc-064646df08f6"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "params = {'max_leaf_nodes': list(range(2, 100)), \n",
    "          'min_samples_split': [2, 3, 4], \n",
    "          'criterion':['entropy', 'gini']}\n",
    "          \n",
    "grid_search_cv = GridSearchCV(DecisionTreeClassifier(random_state=42), params, verbose=1, cv=3)\n",
    "\n",
    "grid_search_cv.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PJeq7G_lSw8p"
   },
   "source": [
    "### Find the best estimator for GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "id": "mOkAtmchS04K",
    "outputId": "ffcb53d3-b2ea-44b1-fa1b-ad45484d29e3"
   },
   "outputs": [],
   "source": [
    "grid_search_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "nhw_p7oN5yGd",
    "outputId": "e9a9af5a-35ec-4725-9384-8baab58c98b8"
   },
   "outputs": [],
   "source": [
    "grid_search_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OeNwIdtk69Uy"
   },
   "outputs": [],
   "source": [
    "# Visualize the Decision Tree (you need to have graphviz installed)\n",
    "from graphviz import Source\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "# Call the export with Graphviz\n",
    "export_graphviz(\n",
    "        tree_clf,\n",
    "        out_file=os.path.join(IMAGES_PATH, \"iris_tree.dot\"),\n",
    "        feature_names=iris.feature_names[2:],\n",
    "        class_names=iris.target_names,\n",
    "        rounded=True,\n",
    "        filled=True\n",
    "    )\n",
    "\n",
    "# Load the file in the notebook\n",
    "Source.from_file(os.path.join(IMAGES_PATH, \"iris_tree.dot\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yLl306XHTFYU"
   },
   "source": [
    "### Visualize the split and the tree growth with the code above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Kd8YJ3JTMdO"
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def plot_decision_boundary(clf, X, y, axes=[0, 7.5, 0, 3], iris=True, legend=False, plot_training=True):\n",
    "    x1s = np.linspace(axes[0], axes[1], 100)\n",
    "    x2s = np.linspace(axes[2], axes[3], 100)\n",
    "    x1, x2 = np.meshgrid(x1s, x2s)\n",
    "    X_new = np.c_[x1.ravel(), x2.ravel()]\n",
    "    y_pred = clf.predict(X_new).reshape(x1.shape)\n",
    "    custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])\n",
    "    plt.contourf(x1, x2, y_pred, alpha=0.3, cmap=custom_cmap)\n",
    "    if not iris:\n",
    "        custom_cmap2 = ListedColormap(['#7d7d58','#4c4c7f','#507d50'])\n",
    "        plt.contour(x1, x2, y_pred, cmap=custom_cmap2, alpha=0.8)\n",
    "    if plot_training:\n",
    "        plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\", label=\"Iris setosa\")\n",
    "        plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\", label=\"Iris versicolor\")\n",
    "        plt.plot(X[:, 0][y==2], X[:, 1][y==2], \"g^\", label=\"Iris virginica\")\n",
    "        plt.axis(axes)\n",
    "    if iris:\n",
    "        plt.xlabel(\"Petal length\", fontsize=14)\n",
    "        plt.ylabel(\"Petal width\", fontsize=14)\n",
    "    else:\n",
    "        plt.xlabel(r\"$x_1$\", fontsize=18)\n",
    "        plt.ylabel(r\"$x_2$\", fontsize=18, rotation=0)\n",
    "    if legend:\n",
    "        plt.legend(loc=\"lower right\", fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oNzAxJRfS8kZ"
   },
   "source": [
    "### Evaluate the accuracy with test\n",
    "What did you get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aC6moaKATAMd"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VBI8yLwGn3ZT"
   },
   "source": [
    "## EXERCISE 2 - My first random forest\n",
    "\n",
    "You are already acquanted with the pitfalls of Decision Trees and might have an intutition of why Random Forest might be a good approach to address those pitfalls.\n",
    "\n",
    "In this exercise you will build your own random forest from scratch. Be creative, make use of for loops or any programmnig tool that you like. Let's go:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DhLPLwWLpV-E"
   },
   "source": [
    "## Part 1 - splitting the dataset to build a forest\n",
    "\n",
    "Go on with the exercise above, use the same datraset. But this time generate 1000 subsets of the **training** dataset, with each containing 100 instances selected in a random manner.\n",
    "\n",
    "You can code it by your own or use [Scikit-Learn ShuffleSplit class](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W0DVPxT2n6eH"
   },
   "outputs": [],
   "source": [
    "# Do your splits here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F4TkWjYZqBzy"
   },
   "source": [
    "## Part 2 - Train each subset!\n",
    "\n",
    "Use the best hyperparameters for the whole moons dataset that you found in the previous exercise. **Train one Decision Tree on each subset** (the subsets you just created in Part 1). \n",
    "\n",
    "Evaluate the 1000 Decision trees you trained over the test set. Create an empty list, and use the method you prefer to loop through each tree, evaluating over the test set and appending the results over the empty list. Once you have your list of accuracies, calculate the mean of all of them.\n",
    "\n",
    "This will most likely perform worse than the Decision Tree you trained with the whole train dataset, achieving not more than 80%. \n",
    "\n",
    "Place your final mean accuracy below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CiEXMl8LqAss"
   },
   "outputs": [],
   "source": [
    "# Train each subset here and print the final mean accuracy over the 1000 trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tcmfJ8FCrrhC"
   },
   "source": [
    "## Part 3 - keeping the most frequent predictions\n",
    "\n",
    "Now we will keep only the most frequent predictions.\n",
    "\n",
    "For each test set **INSTANCE** or **SAMPLE**, generate the predictions of the 1000 Decision trees, and keep only the most frequent ones.\n",
    "\n",
    "For instance:\n",
    "\n",
    "* Test $y_0$ - prediction 1\n",
    "* Test $y_1$ - prediction 1\n",
    "* Test $y_2$ - prediction 0\n",
    "* Test $y_3$ - prediction 1\n",
    "* Test $y_4$ - prediction 1\n",
    "\n",
    "You will keep prediction = 1, as it is the most frequent. You can use SciPy's mode() function to achieve this. \n",
    "\n",
    "Now we have a majority-vote predictions over the whole test set, having the most common predictions for each instance over the test set. Keep this value as an array with the same shape as y_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B4hdOKC5t3-R"
   },
   "outputs": [],
   "source": [
    "# Do your most frequent predictions here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iLPao9oruAh5"
   },
   "source": [
    "## Part 4 - let's see what happens for accuracy!\n",
    "\n",
    "Use the array that you produced above with the most common predictions for each test instance. Compare it with the y_test set using the accuracy_score method.\n",
    "\n",
    "Basically, you should do the following:\n",
    "\n",
    "accuracy_score(my_y_test_dataset, my_most_common_predictions_over_y_dataset)\n",
    "\n",
    "You might need to reshape your most common predictions array so it will be the same shape as the y_test.\n",
    "\n",
    "You should get a slightly higher accuracy, maybe up to 1.5% higher. You just trained your first Random Forest classifier!!! Congratulations! Go an have a beer!\n",
    "\n",
    "But first, comment on why do you think that you are getting a better accuracy. Reflect on Decision Trees instabilities and why creating a Randomized Forest of Decision Trees might yield better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qhVG8euTt7Sa"
   },
   "outputs": [],
   "source": [
    "# Do your final accuracy score here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iYIOkBscwKlx"
   },
   "source": [
    "## Place your reflections here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J2txhFq-wMn5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Session_6_Classical_Data_Analysis_DT_EXERCISES.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
